{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch\n",
    "# faster rcnn model이 포함된 library\n",
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gc\n",
    "\n",
    "from ensemble_boxes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    '''\n",
    "      data_dir: data가 존재하는 폴더 경로\n",
    "      transforms: data transform (resize, crop, Totensor, etc,,,)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, annotation, data_dir, transforms=None):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        # coco annotation 불러오기 (coco API)\n",
    "        self.coco = COCO(annotation)\n",
    "        self.predictions = {\n",
    "            \"images\": self.coco.dataset[\"images\"].copy(),\n",
    "            \"categories\": self.coco.dataset[\"categories\"].copy(),\n",
    "            \"annotations\": None\n",
    "        }\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        \n",
    "        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_info['id'])\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        boxes = np.array([x['bbox'] for x in anns])\n",
    "\n",
    "        # boxex (x_min, y_min, x_max, y_max)\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        \n",
    "        labels = np.array([x['category_id'] for x in anns])\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        areas = np.array([x['area'] for x in anns])\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "                                \n",
    "        is_crowds = np.array([x['iscrowd'] for x in anns])\n",
    "        is_crowds = torch.as_tensor(is_crowds, dtype=torch.int64)\n",
    "                                \n",
    "        segmentation = np.array([x['segmentation'] for x in anns], dtype=object)\n",
    "\n",
    "        target = {'boxes': boxes, 'labels': labels, 'image_id': torch.tensor([index]), 'area': areas,\n",
    "                  'iscrowd': is_crowds}\n",
    "\n",
    "        # transform\n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': labels\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            target['boxes'] = torch.tensor(sample['bboxes'], dtype=torch.float32)\n",
    "\n",
    "        return image, target, image_id\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(512, 512),\n",
    "        A.Flip(p=0.5),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(512, 512),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ensemble_predictions(images):\n",
    "    checkpoints = ['./work_dirs/faster_rcnn_fold0.pth', './work_dirs/faster_rcnn_fold1.pth', './work_dirs/faster_rcnn_fold2.pth', './work_dirs/faster_rcnn_fold3.pth', './work_dirs/faster_rcnn_fold4.pth']\n",
    "    result = []\n",
    "    for checkpoint in checkpoints:\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        images = list(image.float().to(device) for image in images) \n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "        \n",
    "        num_classes = 11  # 11 class + background\n",
    "\n",
    "        # get number of input features for the classifier\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "        # torchvision에 학습되어 있던 RPN 사용\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "        model.to(device)\n",
    "        \n",
    "        model.load_state_dict(torch.load(checkpoint))\n",
    "        model.eval()\n",
    "\n",
    "        outputs = model(images)\n",
    "        result.append(outputs)\n",
    "\n",
    "        del checkpoint, model, device\n",
    "        gc.collect()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_wbf(predictions, image_index, image_size=512, iou_thr=0.55, skip_box_thr=0.5, weights=None):\n",
    "    box = [prediction[image_index]['boxes'].data.cpu().numpy()/(image_size-1) for prediction in predictions]\n",
    "    score = [prediction[image_index]['scores'].data.cpu().numpy() for prediction in predictions]\n",
    "    label = [prediction[image_index]['labels'].data.cpu().numpy() for prediction in predictions]\n",
    "\n",
    "    box, score, label = weighted_boxes_fusion(box, score, label, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
    "    box = box*(image_size-1)\n",
    "\n",
    "    return box, score, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_threshold = 0.05\n",
    "annotation = '../../input/data/test.json'\n",
    "def valid_fn(val_data_loader):\n",
    "    outputs = []\n",
    "    prediction_strings = []\n",
    "    file_names = []\n",
    "    coco = COCO(annotation)\n",
    "\n",
    "    for images, targets, image_ids in tqdm(val_data_loader):\n",
    "        # gpu 계산을 위해 image.to(device)\n",
    "        predictions = make_ensemble_predictions(images)\n",
    "        for i, (image_id, image) in enumerate(zip(image_ids, images)):\n",
    "            prediction_string = ''\n",
    "            image_info = coco.loadImgs(coco.getImgIds(imgIds=image_id[0]))[0]\n",
    "            boxes, scores, labels = run_wbf(predictions, image_index=i)\n",
    "            for box, score, label in zip(boxes, scores, labels):\n",
    "                if score > score_threshold:\n",
    "                        prediction_string += str(int(label)) + ' ' + str(score) + ' ' + str(box[0]) + ' ' + str(\n",
    "                            box[1]) + ' ' + str(box[2]) + ' ' + str(box[3]) + ' '\n",
    "            if prediction_string != '':\n",
    "                prediction_strings.append(prediction_string)\n",
    "                file_names.append(image_info['file_name'])\n",
    "    \n",
    "        del predictions\n",
    "        gc.collect()\n",
    "    \n",
    "\n",
    "    submission = pd.DataFrame()\n",
    "    submission['PredictionString'] = prediction_strings\n",
    "    submission['image_id'] = file_names\n",
    "    submission.to_csv(f'faster_rcnn_submission3.csv', index=None)\n",
    "    print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    data_dir = '../../input/data'\n",
    "    val_dataset = CustomDataset(annotation, data_dir, get_valid_transform())\n",
    "    val_data_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    valid_fn(val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [12:50<00:00,  7.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "                                    PredictionString              image_id\n",
      "0  8 0.9972167611122131 172.1233382821083 235.353...  batch_01_vt/0021.jpg\n",
      "1  8 0.9227560758590698 455.5631074309349 347.568...  batch_01_vt/0028.jpg\n",
      "2  1 0.92191815 82.50074 168.08356 369.84387 368....  batch_01_vt/0031.jpg\n",
      "3  8 0.95953244 32.090534 188.47841 354.94684 374...  batch_01_vt/0032.jpg\n",
      "4  8 0.9960081577301025 146.0392668545246 0.0 355...  batch_01_vt/0070.jpg\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}